{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGirqSE-6FJo",
        "outputId": "8778dea2-3562-497a-9605-757709db6006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.7 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 huggingface-hub-0.7.0 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.5 pyyaml-6.0 requests-2.28.0 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 tokenizers-0.12.1 transformers-4.20.0 wikipedia-api-0.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XSfaEuWN6Iwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8959b42a-f050-4f95-9016-b14b7511818e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-17 05:04:55,149 https://raw.githubusercontent.com/87302380/WEIBO_NER/main/data/weiboNER_2nd_conll_format.train not found in cache, downloading to /tmp/tmpx78152y1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "462545B [00:00, 43442481.61B/s]          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-17 05:04:55,187 copying /tmp/tmpx78152y1 to cache at /root/.flair/datasets/ner_chinese_weibo/weiboNER_2nd_conll_format.train\n",
            "2022-06-17 05:04:55,192 removing temp file /tmp/tmpx78152y1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-17 05:04:55,410 https://raw.githubusercontent.com/87302380/WEIBO_NER/main/data/weiboNER_2nd_conll_format.test not found in cache, downloading to /tmp/tmpz36nv7rm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "94016B [00:00, 28850723.21B/s]          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-17 05:04:55,442 copying /tmp/tmpz36nv7rm to cache at /root/.flair/datasets/ner_chinese_weibo/weiboNER_2nd_conll_format.test\n",
            "2022-06-17 05:04:55,445 removing temp file /tmp/tmpz36nv7rm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-17 05:04:55,657 https://raw.githubusercontent.com/87302380/WEIBO_NER/main/data/weiboNER_2nd_conll_format.dev not found in cache, downloading to /tmp/tmpnagdn2zr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "91510B [00:00, 28164129.66B/s]          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-17 05:04:55,689 copying /tmp/tmpnagdn2zr to cache at /root/.flair/datasets/ner_chinese_weibo/weiboNER_2nd_conll_format.dev\n",
            "2022-06-17 05:04:55,692 removing temp file /tmp/tmpnagdn2zr\n",
            "2022-06-17 05:04:55,694 Reading data from /root/.flair/datasets/ner_chinese_weibo\n",
            "2022-06-17 05:04:55,695 Train: /root/.flair/datasets/ner_chinese_weibo/weiboNER_2nd_conll_format.train\n",
            "2022-06-17 05:04:55,696 Dev: /root/.flair/datasets/ner_chinese_weibo/weiboNER_2nd_conll_format.dev\n",
            "2022-06-17 05:04:55,697 Test: /root/.flair/datasets/ner_chinese_weibo/weiboNER_2nd_conll_format.test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: 1350 train + 270 dev + 270 test sentences\n"
          ]
        }
      ],
      "source": [
        "import flair.datasets\n",
        "from flair.data import Corpus\n",
        "corpus = flair.datasets.NER_CHINESE_WEIBO()\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUdsfL616UWx"
      },
      "outputs": [],
      "source": [
        "import flair\n",
        "from typing import List\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.models import SequenceTagger\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, BertEmbeddings, BytePairEmbeddings\n",
        "\n",
        "tag_type = 'ner'\n",
        "tag_dictionary = corpus.make_label_dictionary(label_type=tag_type)\n",
        "\n",
        "# For an even faster training time, you can comment out the BytePairEmbeddings\n",
        "# Note: there will be a small drop in performance if you do so.\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "    WordEmbeddings('zh-crawl'),\n",
        "    BytePairEmbeddings('zh'),\n",
        "    BertEmbeddings('bert-base-chinese'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=tag_type,\n",
        "                                        use_crf=True)\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train('/content/model/',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              max_epochs=50,\n",
        "              embeddings_storage_mode='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "input_sentences = [\n",
        "  '我 和 华 莱 士 在 美 国 谈 笑 风 生 !',\n",
        "  '狼 王 加 内 特 在 兴 安 岭 看 狼 群 。',\n",
        "  '上 海 自 来 水 来 自 海 上 。',\n",
        "  '马 云 说 阿 里 的 996 工 作 制 是 福 报',\n",
        "  '马 云 和 阿 里 打 拳 击',\n",
        "  '我 带 小 黄 在 上 海 人 民 广 场 的 百 联 商 店 一 起 吃 炸 鸡'\n",
        "]\n",
        "tagger: SequenceTagger = SequenceTagger.load(\"/content/model/final-model.pt\")\n",
        "for input_sentence in input_sentences:\n",
        "  sentence: Sentence = Sentence(input_sentence)\n",
        "  tagger.predict(sentence)\n",
        "  print(sentence.to_tagged_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iARNiC8g_E7O",
        "outputId": "e837f24c-0b40-496b-eee9-a1d782ac98f3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-17 06:20:28,870 loading file /content/model/final-model.pt\n",
            "2022-06-17 06:20:31,691 SequenceTagger predicts: Dictionary with 35 tags: O, S-PER.NOM, B-PER.NOM, E-PER.NOM, I-PER.NOM, S-PER.NAM, B-PER.NAM, E-PER.NAM, I-PER.NAM, S-GPE.NAM, B-GPE.NAM, E-GPE.NAM, I-GPE.NAM, S-ORG.NAM, B-ORG.NAM, E-ORG.NAM, I-ORG.NAM, S-LOC.NAM, B-LOC.NAM, E-LOC.NAM, I-LOC.NAM, S-LOC.NOM, B-LOC.NOM, E-LOC.NOM, I-LOC.NOM, S-ORG.NOM, B-ORG.NOM, E-ORG.NOM, I-ORG.NOM, S-GPE.NOM, B-GPE.NOM, E-GPE.NOM, I-GPE.NOM, <START>, <STOP>\n",
            "Sentence: \"我 和 华 莱 士 在 美 国 谈 笑 风 生 !\" → [\"华 莱 士\"/PER.NAM, \"美 国\"/GPE.NAM]\n",
            "Sentence: \"狼 王 加 内 特 在 兴 安 岭 看 狼 群 。\" → [\"加 内 特\"/PER.NAM, \"兴 安 岭\"/LOC.NAM]\n",
            "Sentence: \"上 海 自 来 水 来 自 海 上 。\" → [\"上 海\"/GPE.NAM]\n",
            "Sentence: \"马 云 说 阿 里 的 996 工 作 制 是 福 报\" → [\"马 云\"/PER.NAM, \"阿 里\"/ORG.NAM]\n",
            "Sentence: \"马 云 和 阿 里 打 拳 击\" → [\"马 云\"/PER.NAM, \"阿 里\"/PER.NAM]\n",
            "Sentence: \"我 带 小 黄 在 上 海 人 民 广 场 的 百 联 商 店 一 起 吃 炸 鸡\" → [\"小 黄\"/PER.NAM, \"上 海\"/GPE.NAM, \"人 民 广 场\"/LOC.NAM, \"百 联 商 店\"/ORG.NAM]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p1oVEcz2_F4b"
      },
      "execution_count": 13,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "中文NER-SequenceTag.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "background_execution": "on"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
